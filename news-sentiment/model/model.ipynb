{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Sentiment Analysis using a BiLSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the tokenizer. Punkt is good for this use case: formal headlines from news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/avinav/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab', quiet=False)\n",
    "gemini = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class\n",
    "\n",
    "The code below contains the custom class that is used to organize the dataset. The dataset contains multiple points of data with each data contianing a headline (string), a date (YYYY-MM-DD), and a tone (float in the range [-1, 1]). The class is used to load the data, and to perform the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineDataset(Dataset):\n",
    "  def __init__(self, headlines, labels, word2vec_model, max_length=128): # max length here is the max number of tokens in a headline\n",
    "    self.headlines = headlines\n",
    "    self.labels = labels\n",
    "    self.word2vec_model = word2vec_model\n",
    "    self.max_length = max_length\n",
    "    self.vector_size = self.word2vec_model.vector_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.headlines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    headline = self.headlines[idx]\n",
    "    sentiment = self.labels[idx]\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(headline.lower())\n",
    "    \n",
    "    # embed using word2vec\n",
    "    vectors = []\n",
    "    for token in tokens[:self.max_length]:\n",
    "      if token in self.word2vec_model.wv:\n",
    "        vectors.append(self.word2vec_model.wv[token])\n",
    "      else:\n",
    "        vectors.append(np.zeros(self.vector_size))\n",
    "    \n",
    "    # pad if needed\n",
    "    if len(vectors) < self.max_length:\n",
    "      vectors.extend([np.zeros(self.vector_size)] * (self.max_length - len(vectors)))\n",
    "    \n",
    "    # convert to tensrs\n",
    "    vectors = torch.FloatTensor(vectors)\n",
    "    sentiment = torch.FloatTensor([sentiment])\n",
    "    \n",
    "    return vectors, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice here is to use a bi-directional LSTM model. This is good for NLP and this task. Chosen as its a good balance between accuracy and computational cost. Something like a transformer model may have performed better but it would have been a lot more complicated and computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMSentiment(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim, output_dim=1, num_layers=2, dropout=0.55):\n",
    "    super(BiLSTMSentiment, self).__init__()\n",
    "    \n",
    "    self.lstm = nn.LSTM(embedding_dim, \n",
    "                        hidden_dim,\n",
    "                        num_layers=num_layers,\n",
    "                        bidirectional=True,\n",
    "                        dropout=dropout if num_layers > 1 else 0,\n",
    "                        batch_first=True)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "  def forward(self, text):\n",
    "    # text shape: [batch size, sequence length, embedding dim]\n",
    "\n",
    "    lstm_output, (hidden, cell) = self.lstm(text) # lstm layers\n",
    "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # dropout layer\n",
    "    \n",
    "    # output layer is tanh to get output in range [-1, 1]\n",
    "    return torch.tanh(self.fc(hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, optimizer, loss_fn, num_epochs=50, patience=6, csv_index=0):\n",
    "  import matplotlib.pyplot as plt\n",
    "  \n",
    "  # load onto cuda or mps if available\n",
    "  if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "  elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "  model.to(device)\n",
    "  print(\"Training model on device: \", device)\n",
    "  \n",
    "  best_val_loss = float('inf')\n",
    "  epochs_no_improve = 0  # Counter for early stopping\n",
    "  early_stop = False\n",
    "  \n",
    "  # For tracking metrics\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  epochs_completed = 0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # training loop\n",
    "    for batch_idx, (headlines, labels) in enumerate(train_loader):\n",
    "        \n",
    "      # move everything to gpu\n",
    "      headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "      # forward pass\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(headlines)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "\n",
    "      # backward pass\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for headlines, labels in val_loader:\n",
    "\n",
    "        # move everything to gpu\n",
    "        headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(headlines)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    epochs_completed = epoch + 1\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*10 + f\" Epoch {epoch+1} \" + \"=\"*10)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check if this is the best model\n",
    "    if val_loss < best_val_loss:\n",
    "      best_val_loss = val_loss\n",
    "      state_folder = 'gemini-states' if gemini else 'states'\n",
    "      torch.save(model.state_dict(), f\"./{state_folder}/model_{csv_index}.pt\")\n",
    "      print(f'\\t Best model saved at epoch {epoch+1}')\n",
    "      epochs_no_improve = 0  # Reset counter\n",
    "    else:\n",
    "      epochs_no_improve += 1\n",
    "      print(f'\\t No improvement for {epochs_no_improve} epochs')\n",
    "      \n",
    "    # Check early stopping condition\n",
    "    if epochs_no_improve >= patience:\n",
    "      print(f'\\n Early stopping triggered after {epoch+1} epochs')\n",
    "      early_stop = True\n",
    "      break\n",
    "  \n",
    "  # Create and save the loss plot with actual epochs completed\n",
    "  epochs = list(range(1, epochs_completed+1))\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "  plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "  plt.title('Training and Validation Loss Over Time')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plot_folder = 'gemini-plots' if gemini else 'plots'\n",
    "  plt.savefig(f'./{plot_folder}/loss_plot_{csv_index}.png')\n",
    "  plt.close()\n",
    "  \n",
    "  if early_stop:\n",
    "    print(\"Training stopped early due to no improvement in validation loss\")\n",
    "  print(f\"Loss plot saved to 'loss_plot.png'\")\n",
    "  print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "  \n",
    "  return train_losses, val_losses, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn, csv_index=0):\n",
    "  # load onto cuda or mps if available\n",
    "  if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "  elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "  # load best trained model\n",
    "  state_folder = \"gemini-states\" if gemini else \"states\"\n",
    "  model.load_state_dict(torch.load(f'./{state_folder}/model_{csv_index}.pt'))\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  predictions = []\n",
    "  actual = []\n",
    "  test_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for headlines, labels in test_loader:\n",
    "      headlines, labels = headlines.to(device), labels.to(device)\n",
    "\n",
    "      outputs = model(headlines)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "\n",
    "      test_loss += loss.item()\n",
    "\n",
    "      predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "      actual.extend(labels.squeeze().cpu().numpy())\n",
    "\n",
    "  test_loss /= len(test_loader)\n",
    "\n",
    "  # get metrics\n",
    "  mse = np.mean((np.array(predictions) - np.array(actual)) ** 2)\n",
    "  mae = np.mean(np.abs(np.array(predictions) - np.array(actual)))\n",
    "\n",
    "  print(f'Test Loss: {test_loss:.4f}')\n",
    "  print(f'MSE: {mse:.4f}')\n",
    "  print(f'MAE: {mae:.4f}')\n",
    "  \n",
    "  return predictions, actual, test_loss, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record df          lr  weight_decay  batch_size  dropout  hidden_dim  num_layers  \\\n",
      "0    0.0005        0.0000          16      0.3          64           2   \n",
      "1    0.0005        0.0000          16      0.3         128           1   \n",
      "2    0.0005        0.0000          16      0.3         128           2   \n",
      "3    0.0005        0.0000          16      0.3         256           1   \n",
      "4    0.0005        0.0000          16      0.3         256           2   \n",
      "..      ...           ...         ...      ...         ...         ...   \n",
      "194  0.0003        0.0010         128      0.3          96           3   \n",
      "195  0.0010        0.0005         128      0.7          32           1   \n",
      "196  0.0050        0.0000         128      0.7         256           3   \n",
      "197  0.0020        0.0050          64      0.4         512           3   \n",
      "198  0.0050        0.0010          16      0.2          96           1   \n",
      "\n",
      "     val_loss  test_loss       mae  \n",
      "0    0.052361   0.050499  0.165377  \n",
      "1    0.048962   0.047763  0.155124  \n",
      "2    0.052827   0.051820  0.163707  \n",
      "3         NaN        NaN       NaN  \n",
      "4         NaN        NaN       NaN  \n",
      "..        ...        ...       ...  \n",
      "194       NaN        NaN       NaN  \n",
      "195       NaN        NaN       NaN  \n",
      "196       NaN        NaN       NaN  \n",
      "197       NaN        NaN       NaN  \n",
      "198       NaN        NaN       NaN  \n",
      "\n",
      "[199 rows x 9 columns]\n",
      "Starting hyperparameter evaluations from row 3\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating configuration 4/199:\n",
      "lr=0.0005, weight_decay=0.0, batch_size=16, dropout=0.3, hidden_dim=256, num_layers=1\n",
      "==================================================\n",
      "Training model on device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/gq18gwmj16qb1scxzrddbncr0000gn/T/ipykernel_26692/136434337.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  vectors = torch.FloatTensor(vectors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Training Loss: 0.0696\n",
      "Validation Loss: 0.0659\n",
      "\t Best model saved at epoch 1\n",
      "\n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Training Loss: 0.0612\n",
      "Validation Loss: 0.0628\n",
      "\t Best model saved at epoch 2\n",
      "\n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Training Loss: 0.0569\n",
      "Validation Loss: 0.0599\n",
      "\t Best model saved at epoch 3\n",
      "\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Training Loss: 0.0542\n",
      "Validation Loss: 0.0605\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Training Loss: 0.0507\n",
      "Validation Loss: 0.0565\n",
      "\t Best model saved at epoch 5\n",
      "\n",
      "\n",
      "========== Epoch 6 ==========\n",
      "Training Loss: 0.0478\n",
      "Validation Loss: 0.0566\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 7 ==========\n",
      "Training Loss: 0.0446\n",
      "Validation Loss: 0.0561\n",
      "\t Best model saved at epoch 7\n",
      "\n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Training Loss: 0.0417\n",
      "Validation Loss: 0.0542\n",
      "\t Best model saved at epoch 8\n",
      "\n",
      "\n",
      "========== Epoch 9 ==========\n",
      "Training Loss: 0.0380\n",
      "Validation Loss: 0.0520\n",
      "\t Best model saved at epoch 9\n",
      "\n",
      "\n",
      "========== Epoch 10 ==========\n",
      "Training Loss: 0.0344\n",
      "Validation Loss: 0.0542\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 11 ==========\n",
      "Training Loss: 0.0309\n",
      "Validation Loss: 0.0510\n",
      "\t Best model saved at epoch 11\n",
      "\n",
      "\n",
      "========== Epoch 12 ==========\n",
      "Training Loss: 0.0275\n",
      "Validation Loss: 0.0510\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 13 ==========\n",
      "Training Loss: 0.0250\n",
      "Validation Loss: 0.0524\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 14 ==========\n",
      "Training Loss: 0.0218\n",
      "Validation Loss: 0.0494\n",
      "\t Best model saved at epoch 14\n",
      "\n",
      "\n",
      "========== Epoch 15 ==========\n",
      "Training Loss: 0.0194\n",
      "Validation Loss: 0.0527\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 16 ==========\n",
      "Training Loss: 0.0171\n",
      "Validation Loss: 0.0511\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 17 ==========\n",
      "Training Loss: 0.0156\n",
      "Validation Loss: 0.0520\n",
      "\t No improvement for 3 epochs\n",
      "\n",
      "\n",
      "========== Epoch 18 ==========\n",
      "Training Loss: 0.0143\n",
      "Validation Loss: 0.0525\n",
      "\t No improvement for 4 epochs\n",
      "\n",
      "\n",
      "========== Epoch 19 ==========\n",
      "Training Loss: 0.0127\n",
      "Validation Loss: 0.0533\n",
      "\t No improvement for 5 epochs\n",
      "\n",
      "\n",
      "========== Epoch 20 ==========\n",
      "Training Loss: 0.0114\n",
      "Validation Loss: 0.0541\n",
      "\t No improvement for 6 epochs\n",
      "\n",
      " Early stopping triggered after 20 epochs\n",
      "Training stopped early due to no improvement in validation loss\n",
      "Loss plot saved to 'loss_plot.png'\n",
      "Best validation loss: 0.0494\n",
      "Test Loss: 0.0468\n",
      "MSE: 0.0467\n",
      "MAE: 0.1502\n",
      "\n",
      "Results for configuration 4:\n",
      "Best validation loss: 0.049353\n",
      "Test loss: 0.046830\n",
      "MAE: 0.150216\n",
      "Results saved to ./gemini_record.csv\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating configuration 5/199:\n",
      "lr=0.0005, weight_decay=0.0, batch_size=16, dropout=0.3, hidden_dim=256, num_layers=2\n",
      "==================================================\n",
      "Training model on device:  mps\n",
      "\n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Training Loss: 0.0685\n",
      "Validation Loss: 0.0656\n",
      "\t Best model saved at epoch 1\n",
      "\n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Training Loss: 0.0605\n",
      "Validation Loss: 0.0607\n",
      "\t Best model saved at epoch 2\n",
      "\n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Training Loss: 0.0562\n",
      "Validation Loss: 0.0578\n",
      "\t Best model saved at epoch 3\n",
      "\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Training Loss: 0.0522\n",
      "Validation Loss: 0.0666\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Training Loss: 0.0488\n",
      "Validation Loss: 0.0550\n",
      "\t Best model saved at epoch 5\n",
      "\n",
      "\n",
      "========== Epoch 6 ==========\n",
      "Training Loss: 0.0448\n",
      "Validation Loss: 0.0550\n",
      "\t Best model saved at epoch 6\n",
      "\n",
      "\n",
      "========== Epoch 7 ==========\n",
      "Training Loss: 0.0407\n",
      "Validation Loss: 0.0542\n",
      "\t Best model saved at epoch 7\n",
      "\n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Training Loss: 0.0371\n",
      "Validation Loss: 0.0566\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 9 ==========\n",
      "Training Loss: 0.0329\n",
      "Validation Loss: 0.0544\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 10 ==========\n",
      "Training Loss: 0.0289\n",
      "Validation Loss: 0.0536\n",
      "\t Best model saved at epoch 10\n",
      "\n",
      "\n",
      "========== Epoch 11 ==========\n",
      "Training Loss: 0.0256\n",
      "Validation Loss: 0.0567\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 12 ==========\n",
      "Training Loss: 0.0226\n",
      "Validation Loss: 0.0531\n",
      "\t Best model saved at epoch 12\n",
      "\n",
      "\n",
      "========== Epoch 13 ==========\n",
      "Training Loss: 0.0194\n",
      "Validation Loss: 0.0514\n",
      "\t Best model saved at epoch 13\n",
      "\n",
      "\n",
      "========== Epoch 14 ==========\n",
      "Training Loss: 0.0168\n",
      "Validation Loss: 0.0533\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 15 ==========\n",
      "Training Loss: 0.0144\n",
      "Validation Loss: 0.0569\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 16 ==========\n",
      "Training Loss: 0.0136\n",
      "Validation Loss: 0.0550\n",
      "\t No improvement for 3 epochs\n",
      "\n",
      "\n",
      "========== Epoch 17 ==========\n",
      "Training Loss: 0.0116\n",
      "Validation Loss: 0.0575\n",
      "\t No improvement for 4 epochs\n",
      "\n",
      "\n",
      "========== Epoch 18 ==========\n",
      "Training Loss: 0.0108\n",
      "Validation Loss: 0.0549\n",
      "\t No improvement for 5 epochs\n",
      "\n",
      "\n",
      "========== Epoch 19 ==========\n",
      "Training Loss: 0.0106\n",
      "Validation Loss: 0.0551\n",
      "\t No improvement for 6 epochs\n",
      "\n",
      " Early stopping triggered after 19 epochs\n",
      "Training stopped early due to no improvement in validation loss\n",
      "Loss plot saved to 'loss_plot.png'\n",
      "Best validation loss: 0.0514\n",
      "Test Loss: 0.0490\n",
      "MSE: 0.0488\n",
      "MAE: 0.1527\n",
      "\n",
      "Results for configuration 5:\n",
      "Best validation loss: 0.051417\n",
      "Test loss: 0.048987\n",
      "MAE: 0.152719\n",
      "Results saved to ./gemini_record.csv\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating configuration 6/199:\n",
      "lr=0.0005, weight_decay=0.0, batch_size=16, dropout=0.5, hidden_dim=64, num_layers=1\n",
      "==================================================\n",
      "Training model on device:  mps\n",
      "\n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Training Loss: 0.0729\n",
      "Validation Loss: 0.0639\n",
      "\t Best model saved at epoch 1\n",
      "\n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Training Loss: 0.0623\n",
      "Validation Loss: 0.0603\n",
      "\t Best model saved at epoch 2\n",
      "\n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Training Loss: 0.0589\n",
      "Validation Loss: 0.0625\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Training Loss: 0.0566\n",
      "Validation Loss: 0.0583\n",
      "\t Best model saved at epoch 4\n",
      "\n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Training Loss: 0.0541\n",
      "Validation Loss: 0.0568\n",
      "\t Best model saved at epoch 5\n",
      "\n",
      "\n",
      "========== Epoch 6 ==========\n",
      "Training Loss: 0.0523\n",
      "Validation Loss: 0.0583\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 7 ==========\n",
      "Training Loss: 0.0506\n",
      "Validation Loss: 0.0549\n",
      "\t Best model saved at epoch 7\n",
      "\n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Training Loss: 0.0483\n",
      "Validation Loss: 0.0579\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 9 ==========\n",
      "Training Loss: 0.0464\n",
      "Validation Loss: 0.0544\n",
      "\t Best model saved at epoch 9\n",
      "\n",
      "\n",
      "========== Epoch 10 ==========\n",
      "Training Loss: 0.0441\n",
      "Validation Loss: 0.0533\n",
      "\t Best model saved at epoch 10\n",
      "\n",
      "\n",
      "========== Epoch 11 ==========\n",
      "Training Loss: 0.0428\n",
      "Validation Loss: 0.0528\n",
      "\t Best model saved at epoch 11\n",
      "\n",
      "\n",
      "========== Epoch 12 ==========\n",
      "Training Loss: 0.0417\n",
      "Validation Loss: 0.0544\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 13 ==========\n",
      "Training Loss: 0.0394\n",
      "Validation Loss: 0.0526\n",
      "\t Best model saved at epoch 13\n",
      "\n",
      "\n",
      "========== Epoch 14 ==========\n",
      "Training Loss: 0.0376\n",
      "Validation Loss: 0.0518\n",
      "\t Best model saved at epoch 14\n",
      "\n",
      "\n",
      "========== Epoch 15 ==========\n",
      "Training Loss: 0.0359\n",
      "Validation Loss: 0.0527\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 16 ==========\n",
      "Training Loss: 0.0343\n",
      "Validation Loss: 0.0517\n",
      "\t Best model saved at epoch 16\n",
      "\n",
      "\n",
      "========== Epoch 17 ==========\n",
      "Training Loss: 0.0333\n",
      "Validation Loss: 0.0509\n",
      "\t Best model saved at epoch 17\n",
      "\n",
      "\n",
      "========== Epoch 18 ==========\n",
      "Training Loss: 0.0318\n",
      "Validation Loss: 0.0499\n",
      "\t Best model saved at epoch 18\n",
      "\n",
      "\n",
      "========== Epoch 19 ==========\n",
      "Training Loss: 0.0302\n",
      "Validation Loss: 0.0511\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 20 ==========\n",
      "Training Loss: 0.0298\n",
      "Validation Loss: 0.0523\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 21 ==========\n",
      "Training Loss: 0.0280\n",
      "Validation Loss: 0.0512\n",
      "\t No improvement for 3 epochs\n",
      "\n",
      "\n",
      "========== Epoch 22 ==========\n",
      "Training Loss: 0.0270\n",
      "Validation Loss: 0.0506\n",
      "\t No improvement for 4 epochs\n",
      "\n",
      "\n",
      "========== Epoch 23 ==========\n",
      "Training Loss: 0.0266\n",
      "Validation Loss: 0.0494\n",
      "\t Best model saved at epoch 23\n",
      "\n",
      "\n",
      "========== Epoch 24 ==========\n",
      "Training Loss: 0.0245\n",
      "Validation Loss: 0.0495\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 25 ==========\n",
      "Training Loss: 0.0239\n",
      "Validation Loss: 0.0515\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 26 ==========\n",
      "Training Loss: 0.0236\n",
      "Validation Loss: 0.0505\n",
      "\t No improvement for 3 epochs\n",
      "\n",
      "\n",
      "========== Epoch 27 ==========\n",
      "Training Loss: 0.0226\n",
      "Validation Loss: 0.0543\n",
      "\t No improvement for 4 epochs\n",
      "\n",
      "\n",
      "========== Epoch 28 ==========\n",
      "Training Loss: 0.0210\n",
      "Validation Loss: 0.0488\n",
      "\t Best model saved at epoch 28\n",
      "\n",
      "\n",
      "========== Epoch 29 ==========\n",
      "Training Loss: 0.0207\n",
      "Validation Loss: 0.0526\n",
      "\t No improvement for 1 epochs\n",
      "\n",
      "\n",
      "========== Epoch 30 ==========\n",
      "Training Loss: 0.0201\n",
      "Validation Loss: 0.0550\n",
      "\t No improvement for 2 epochs\n",
      "\n",
      "\n",
      "========== Epoch 31 ==========\n",
      "Training Loss: 0.0194\n",
      "Validation Loss: 0.0517\n",
      "\t No improvement for 3 epochs\n",
      "\n",
      "\n",
      "========== Epoch 32 ==========\n",
      "Training Loss: 0.0186\n",
      "Validation Loss: 0.0541\n",
      "\t No improvement for 4 epochs\n",
      "\n",
      "\n",
      "========== Epoch 33 ==========\n",
      "Training Loss: 0.0184\n",
      "Validation Loss: 0.0516\n",
      "\t No improvement for 5 epochs\n",
      "\n",
      "\n",
      "========== Epoch 34 ==========\n",
      "Training Loss: 0.0178\n",
      "Validation Loss: 0.0514\n",
      "\t No improvement for 6 epochs\n",
      "\n",
      " Early stopping triggered after 34 epochs\n",
      "Training stopped early due to no improvement in validation loss\n",
      "Loss plot saved to 'loss_plot.png'\n",
      "Best validation loss: 0.0488\n",
      "Test Loss: 0.0458\n",
      "MSE: 0.0457\n",
      "MAE: 0.1501\n",
      "\n",
      "Results for configuration 6:\n",
      "Best validation loss: 0.048795\n",
      "Test loss: 0.045824\n",
      "MAE: 0.150129\n",
      "Results saved to ./gemini_record.csv\n",
      "\n",
      "\n",
      "==================================================\n",
      "Evaluating configuration 7/199:\n",
      "lr=0.0005, weight_decay=0.0, batch_size=16, dropout=0.5, hidden_dim=64, num_layers=2\n",
      "==================================================\n",
      "Training model on device:  mps\n",
      "\n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Training Loss: 0.0705\n",
      "Validation Loss: 0.0634\n",
      "\t Best model saved at epoch 1\n",
      "\n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Training Loss: 0.0616\n",
      "Validation Loss: 0.0607\n",
      "\t Best model saved at epoch 2\n",
      "\n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Training Loss: 0.0580\n",
      "Validation Loss: 0.0591\n",
      "\t Best model saved at epoch 3\n",
      "\n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Training Loss: 0.0551\n",
      "Validation Loss: 0.0581\n",
      "\t Best model saved at epoch 4\n",
      "\n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Training Loss: 0.0526\n",
      "Validation Loss: 0.0566\n",
      "\t Best model saved at epoch 5\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  # Check if record.csv exists and load it\n",
    "  csv_file_path = './gemini_record.csv' if gemini else './record.csv'\n",
    "  if not os.path.exists(csv_file_path):\n",
    "    print(f\"Error: ./{csv_file_path} not found.\")\n",
    "    return\n",
    "  \n",
    "  record_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "  print(\"record df\", record_df)\n",
    "  \n",
    "  # Find the first row with missing evaluation metrics\n",
    "  next_row = None\n",
    "  for idx, row in record_df.iterrows():\n",
    "    if pd.isna(row['val_loss']) or pd.isna(row['test_loss']) or pd.isna(row['mae']):\n",
    "      next_row = idx\n",
    "      break\n",
    "  \n",
    "  if next_row is None:\n",
    "    print(\"All hyperparameter configurations have already been evaluated.\")\n",
    "    return\n",
    "    \n",
    "  print(f\"Starting hyperparameter evaluations from row {next_row}\")\n",
    "\n",
    "  # arbritrary start\n",
    "  next_row = 10\n",
    "  \n",
    "  # Process each row that needs evaluation\n",
    "  for idx in range(next_row, len(record_df)):\n",
    "    row = record_df.iloc[idx]\n",
    "    \n",
    "    # Skip row if it has already been evaluated\n",
    "    if row['val_loss'] is not None:\n",
    "      continue\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    lr = row['lr']\n",
    "    weight_decay = row['weight_decay']\n",
    "    batch_size = int(row['batch_size'])\n",
    "    dropout = row['dropout']\n",
    "    hidden_dim = int(row['hidden_dim'])\n",
    "    num_layers = int(row['num_layers'])\n",
    "    \n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(f\"Evaluating configuration {idx+1}/{len(record_df)}:\")\n",
    "    print(f\"lr={lr}, weight_decay={weight_decay}, batch_size={batch_size}, dropout={dropout}, hidden_dim={hidden_dim}, num_layers={num_layers}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # loading the data\n",
    "    label_file_name = \"gemini\" if gemini else \"data\"\n",
    "    df = pd.read_csv(f'../data-collection/{label_file_name}.csv')\n",
    "\n",
    "    # tokenize the headlines for word2vec to process\n",
    "    tokenized_headlines = [word_tokenize(str(headline).lower()) for headline in df['headline']]  \n",
    "\n",
    "    # train word2vec model\n",
    "    word2vec_model = Word2Vec(sentences=tokenized_headlines, vector_size=200, window=4, min_count=1, workers=6, epochs=15, sg=1)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(df['headline'], df['label'], test_size=0.3, random_state=7415)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=7415)\n",
    "\n",
    "    # dataset creation\n",
    "    train_dataset = HeadlineDataset(X_train.values, y_train.values, word2vec_model)\n",
    "    valid_dataset = HeadlineDataset(X_valid.values, y_valid.values, word2vec_model)\n",
    "    test_dataset = HeadlineDataset(X_test.values, y_test.values, word2vec_model)  \n",
    "\n",
    "    # dataloader creation\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # initialize model with current hyperparameters\n",
    "    embedding_dim = word2vec_model.vector_size \n",
    "    model = BiLSTMSentiment(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    # loss function & optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Train model with fixed patience of 6\n",
    "    _, _, best_val_loss = train_model(model, train_loader, valid_loader, test_loader, \n",
    "                                      optimizer, loss_fn, num_epochs=50, patience=6, csv_index=idx)\n",
    "\n",
    "    # Test the model and get metrics\n",
    "    _, _, test_loss, mae = test_model(model, test_loader, loss_fn, idx)\n",
    "    \n",
    "    # Update the CSV with results\n",
    "    record_df.at[idx, 'val_loss'] = round(best_val_loss, 6)\n",
    "    record_df.at[idx, 'test_loss'] = round(test_loss, 6)\n",
    "    record_df.at[idx, 'mae'] = round(mae, 6)\n",
    "    \n",
    "    # Save after each evaluation to preserve progress\n",
    "    record_df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"\\nResults for configuration {idx+1}:\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Test loss: {test_loss:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"Results saved to {csv_file_path}\")\n",
    "  \n",
    "  print(\"\\nAll hyperparameter configurations have been evaluated!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
