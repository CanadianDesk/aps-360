{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Sentiment Analysis using a BiLSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the tokenizer. Punkt is good for this use case: formal headlines from news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/avinav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "The code below contains the custom class that is used to organize the dataset. The dataset contains multiple points of data with each data contianing a headline (string), a date (YYYY-MM-DD), and a tone (float in the range [-1, 1]). The class is used to load the data, and to perform the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineDataset(Dataset):\n",
    "  def __init__(self, headlines, labels, word2vec_model, max_length=128):\n",
    "    self.headlines = headlines\n",
    "    self.labels = labels\n",
    "    self.word2vecmodel = word2vec_model\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.headlines)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    headline = self.headlines[idx]\n",
    "    sentiment = self.sentiments[idx]\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(headline.lower())\n",
    "    \n",
    "    # embed using word2vec\n",
    "    vectors = []\n",
    "    for token in tokens[:self.max_length]:\n",
    "      if token in self.word2vec_model.wv:\n",
    "        vectors.append(self.word2vec_model.wv[token])\n",
    "      else:\n",
    "        vectors.append(np.zeros(self.vector_size))\n",
    "    \n",
    "    # pad if needed\n",
    "    if len(vectors) < self.max_length:\n",
    "      vectors.extend([np.zeros(self.vector_size)] * (self.max_length - len(vectors)))\n",
    "    \n",
    "    # convert to tensrs\n",
    "    vectors = torch.FloatTensor(vectors)\n",
    "    sentiment = torch.FloatTensor([sentiment])\n",
    "    \n",
    "    return vectors, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice here is to use a bi-directional LSTM model. This is good for NLP and this task. Chosen as its a good balance between accuracy and computational cost. Something like a transformer model may have performed better but it would have been a lot more complicated and computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMSentiment(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim, output_dim=1, num_layers=2, dropout=0.5):\n",
    "    super(BiLSTMSentiment, self).__init__()\n",
    "    \n",
    "    self.lstm = nn.LSTM(embedding_dim, \n",
    "                        hidden_dim,\n",
    "                        num_layers=num_layers,\n",
    "                        bidirectional=True,\n",
    "                        dropout=dropout if num_layers > 1 else 0,\n",
    "                        batch_first=True)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "  def forward(self, text):\n",
    "    # text shape: [batch size, sequence length, embedding dim]\n",
    "\n",
    "    lstm_output, (hidden, cell) = self.lstm(text) # lstm layers\n",
    "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # dropout layer\n",
    "    \n",
    "    # output layer is tanh to get output in range [-1, 1]\n",
    "    return torch.tanh(self.fc(hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
